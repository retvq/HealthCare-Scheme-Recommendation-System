{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "class HealthcareRAG:\n",
    "    def __init__(self):\n",
    "        # Initialize embedding model - runs on CPU\n",
    "        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize ChromaDB\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        \n",
    "        # Create collections for different data types\n",
    "        self.patient_collection = self.chroma_client.create_collection(\"patient_data\")\n",
    "        self.schemes_collection = self.chroma_client.create_collection(\"schemes_data\")\n",
    "        \n",
    "        # Initialize tokenizer and model (example with Phi-2)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/phi-2\",\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=\"cpu\"\n",
    "        )\n",
    "\n",
    "    def process_patient_data(self, data_csv):\n",
    "        df = pd.read_csv(data_csv)\n",
    "        \n",
    "        # Process each category\n",
    "        for _, row in df.iterrows():\n",
    "            # Create time series analysis for each category\n",
    "            category_data = {\n",
    "                'category': row['Category'],\n",
    "                'category_name': row['Category Name'],\n",
    "                'data_code': row['Data Code'],\n",
    "                'data_name': row['Data Name']\n",
    "            }\n",
    "            \n",
    "            # Calculate trend\n",
    "            time_series = row.iloc[4:].astype(float)  # Starting from April 2023\n",
    "            category_data['trend'] = self.calculate_trend(time_series)\n",
    "            \n",
    "            # Store in ChromaDB\n",
    "            self.patient_collection.add(\n",
    "                documents=[str(category_data)],\n",
    "                metadatas=[category_data],\n",
    "                ids=[f\"patient_{row['Category']}_{row['Data Code']}\"]\n",
    "            )\n",
    "\n",
    "    def process_schemes_data(self, schemes_csv):\n",
    "        df = pd.read_csv(schemes_csv)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            scheme_data = row.to_dict()\n",
    "            \n",
    "            # Store in ChromaDB\n",
    "            self.schemes_collection.add(\n",
    "                documents=[str(scheme_data)],\n",
    "                metadatas=[scheme_data],\n",
    "                ids=[f\"scheme_{row['Category']}_{row['Scheme Name']}\"]\n",
    "            )\n",
    "\n",
    "    def calculate_trend(self, time_series):\n",
    "        # Calculate basic trend indicators\n",
    "        mean = time_series.mean()\n",
    "        std = time_series.std()\n",
    "        growth_rate = (time_series.iloc[-1] - time_series.iloc[0]) / time_series.iloc[0] * 100\n",
    "        \n",
    "        return {\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'growth_rate': growth_rate,\n",
    "            'trend_direction': 'increasing' if growth_rate > 0 else 'decreasing'\n",
    "        }\n",
    "\n",
    "    def analyze_category(self, category_name):\n",
    "        # Retrieve relevant patient data\n",
    "        patient_results = self.patient_collection.query(\n",
    "            query_texts=[category_name],\n",
    "            n_results=5\n",
    "        )\n",
    "        \n",
    "        # Retrieve relevant schemes\n",
    "        scheme_results = self.schemes_collection.query(\n",
    "            query_texts=[category_name],\n",
    "            n_results=5\n",
    "        )\n",
    "        \n",
    "        # Prepare prompt for analysis\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following healthcare category: {category_name}\n",
    "        \n",
    "        Patient Data Trends:\n",
    "        {patient_results}\n",
    "        \n",
    "        Related Government Schemes:\n",
    "        {scheme_results}\n",
    "        \n",
    "        Please provide:\n",
    "        1. Summary of patient trends\n",
    "        2. Analysis of existing schemes' effectiveness\n",
    "        3. Suggestions for new schemes based on trends\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate analysis using local LLM\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=1000,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def suggest_new_scheme(self, category_name, trend_analysis):\n",
    "        # Create prompt for scheme suggestion\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following trend analysis for {category_name}:\n",
    "        {trend_analysis}\n",
    "        \n",
    "        Suggest a new government scheme that addresses:\n",
    "        1. Current gaps in coverage\n",
    "        2. Emerging trends in patient needs\n",
    "        3. Implementation strategy\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=800,\n",
    "            temperature=0.8,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e836f73f45ae4b58963472655218d792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritvi\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ritvi\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc891bfa5a34f56854f9fd243f39218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda8ef0dfe374ea190b4ff31b7e37d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6488a4248e36432585349aea9b79a939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde5bc1a03a047579f9d3c26cb7577c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa9596ab85247c4b207e33421a19bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb81fe9972041278f90f79ec036f3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the system\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rag_system \u001b[38;5;241m=\u001b[39m \u001b[43mHealthcareRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Process your data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m rag_system\u001b[38;5;241m.\u001b[39mprocess_patient_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m, in \u001b[0;36mHealthcareRAG.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize tokenizer and model (example with Phi-2)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py:3577\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3575\u001b[0m         )\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 3577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   3578\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3579\u001b[0m         )\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[1;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "rag_system = HealthcareRAG()\n",
    "\n",
    "# Process your data\n",
    "rag_system.process_patient_data('Data.csv')\n",
    "rag_system.process_schemes_data('schemes.csv')\n",
    "\n",
    "# Analyze a specific category\n",
    "analysis = rag_system.analyze_category('Pregnancy')\n",
    "new_scheme = rag_system.suggest_new_scheme('Pregnancy', analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
